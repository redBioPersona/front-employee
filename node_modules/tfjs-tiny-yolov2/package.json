{
  "_args": [
    [
      {
        "raw": "tfjs-tiny-yolov2@^0.4.0",
        "scope": null,
        "escapedName": "tfjs-tiny-yolov2",
        "name": "tfjs-tiny-yolov2",
        "rawSpec": "^0.4.0",
        "spec": ">=0.4.0 <0.5.0",
        "type": "range"
      },
      "D:\\webFinger\\MeteorMBES\\node_modules\\face-api.js"
    ]
  ],
  "_from": "tfjs-tiny-yolov2@^0.4.0",
  "_hasShrinkwrap": false,
  "_id": "tfjs-tiny-yolov2@0.4.0",
  "_location": "/tfjs-tiny-yolov2",
  "_nodeVersion": "9.2.0",
  "_npmOperationalInternal": {
    "host": "s3://npm-registry-packages",
    "tmp": "tmp/tfjs-tiny-yolov2_0.4.0_1544689868370_0.8156630792071509"
  },
  "_npmUser": {
    "name": "justadudewhohacks",
    "email": "muehler.v@gmail.com"
  },
  "_npmVersion": "6.4.1",
  "_phantomChildren": {},
  "_requested": {
    "raw": "tfjs-tiny-yolov2@^0.4.0",
    "scope": null,
    "escapedName": "tfjs-tiny-yolov2",
    "name": "tfjs-tiny-yolov2",
    "rawSpec": "^0.4.0",
    "spec": ">=0.4.0 <0.5.0",
    "type": "range"
  },
  "_requiredBy": [
    "/face-api.js"
  ],
  "_resolved": "https://registry.npmjs.org/tfjs-tiny-yolov2/-/tfjs-tiny-yolov2-0.4.0.tgz",
  "_shasum": "9e4f269bca09e2dd09567e92c6fbbc0ed0ee400c",
  "_shrinkwrap": null,
  "_spec": "tfjs-tiny-yolov2@^0.4.0",
  "_where": "D:\\webFinger\\MeteorMBES\\node_modules\\face-api.js",
  "author": {
    "name": "justadudewhohacks"
  },
  "dependencies": {
    "@tensorflow/tfjs-core": "0.14.2",
    "tfjs-image-recognition-base": "^0.3.0",
    "tslib": "^1.9.3"
  },
  "description": "Tiny YOLO v2 object detection with tensorflow.js.",
  "devDependencies": {
    "@types/jasmine": "^2.8.8",
    "@types/node": "^10.12.12",
    "jasmine-core": "^3.2.1",
    "karma": "^3.0.0",
    "karma-chrome-launcher": "^2.2.0",
    "karma-jasmine": "^1.1.2",
    "karma-typescript": "^3.0.12",
    "rollup": "^0.65.0",
    "rollup-plugin-commonjs": "^9.1.6",
    "rollup-plugin-node-resolve": "^3.3.0",
    "rollup-plugin-typescript2": "^0.16.1",
    "rollup-plugin-uglify": "^4.0.0",
    "typescript": "2.8.4"
  },
  "directories": {},
  "dist": {
    "integrity": "sha512-jCsAB9cn3TCikiYr1i4rqBsS5onPJ4E6VnxfRiqHGZRC4crWZ3blmyPPOiVUQx+UXMJQ77rEWfiVQE0/84VmtA==",
    "shasum": "9e4f269bca09e2dd09567e92c6fbbc0ed0ee400c",
    "tarball": "https://registry.npmjs.org/tfjs-tiny-yolov2/-/tfjs-tiny-yolov2-0.4.0.tgz",
    "fileCount": 159,
    "unpackedSize": 2782013,
    "npm-signature": "-----BEGIN PGP SIGNATURE-----\r\nVersion: OpenPGP.js v3.0.4\r\nComment: https://openpgpjs.org\r\n\r\nwsFcBAEBCAAQBQJcEhjNCRA9TVsSAnZWagAAtmkQAIH+/JHD3PEDyiDb9zEi\nyAzSsKB1c+Jt79gJ2KhWyjI25CUKd/ik5IWEG9BCJX2OR0bfCMjKI/bjlvtE\nc1ZCLQrWv9fFzCP5gYO8ibpJn9ArQNw/F3Xk+BacP37wdIyKKQ72SPN8H8HP\nNMUUYNttM8BNKDqpQnO3yccIkbUEvnyHObJtLYSQUOVuxzlPT3bY7D1sOICP\nQQ81xxI0LczJGIeZO38rx6Aux6eenTxftCE3X7iTnqxS5Pd0WaPybzy1mpPO\nJYj8GwcrOeFK9Jn6Yqc0nnY6XROsx5Fw3zRuAaCnjzeLOUH1IV9NRCZce8J9\nEqVvKrrW8IY+b2UdPDTT26eezgAV6PWe3u7a2Gj6YIpj7xRtBXQ7Gwt0HtCL\nq1w92MvMuI9gkhAPYOTShpY2Ch0MUCFu7AmzCTFABEA4joc3Y3pdzZHCIvn+\nIcNNvduk0aDM7Acy9haKG9LO51bvU1qCxsScUh2vdSjzOVKgc5acWQ4pmqui\nYAuwU57xgAgMVmSz004WvEI7EIRIq9BzwO0x0J9Sldt78IpRN3fx2fwbBsy9\nKPLeZPs3rJBSHVKIwPpHBN27/VTphLL83gwjNrksQgUIaimUbu7OGcCD8kjv\nZeGgJkpIgfYwxlC4TUbvofMMepwqn3Wp/nA86iXzrOJhLxuT5RrZgxsNi0Lz\nAuDC\r\n=DHYu\r\n-----END PGP SIGNATURE-----\r\n"
  },
  "gitHead": "0ddad18731379ea214e0cfa0238b83b29dfbdb16",
  "keywords": [
    "tiny",
    "yolo",
    "tensorflow",
    "tf"
  ],
  "license": "MIT",
  "main": "./build/commonjs/index.js",
  "maintainers": [
    {
      "name": "justadudewhohacks",
      "email": "muehler.v@gmail.com"
    }
  ],
  "module": "./build/es6/index.js",
  "name": "tfjs-tiny-yolov2",
  "optionalDependencies": {},
  "readme": "# tfjs-tiny-yolov2\r\n\r\n[![Build Status](https://travis-ci.org/justadudewhohacks/tfjs-tiny-yolov2.svg?branch=master)](https://travis-ci.org/justadudewhohacks/tfjs-tiny-yolov2)\r\n\r\n**JavaScript object detection in the browser based on a tensorflow.js implementation of tiny yolov2.**\r\n\r\nTable of Contents:\r\n\r\n* **[Pre Trained Models](#pre-trained-models)**\r\n* **[Running the Examples](#running-the-examples)**\r\n* **[Usage](#usage)**\r\n* **[Training your own Object Detector](#training-your-own-object-detector)**\r\n  * **[Defining your Model Config](#defining-your-model-config)**\r\n  * **[Labeling your Data with Ground Truth Boxes](#labeling-your-data-with-ground-truth-boxes)**\r\n  * **[Computing Box Anchors](#computing-box-anchors)**\r\n  * **[Yolo Loss Function](#yolo-loss-function)**\r\n  * **[Initializing the Model Weights](#initializing-the-model-weights)**\r\n  * **[Start Training](#start-training)**\r\n  * **[Overfit first!](#overfit-first)**\r\n\r\n<a name=\"pre-trained-models\"></a>\r\n\r\n## Pre Trained Models\r\n\r\nThe VOC and COCO models correspond to the quantized weights from the official [darknet](https://github.com/pjreddie/darknet) repo. The face detector uses depthwise separable convolutions instead of regular convolutions allowing for much faster prediction and a tiny model size, which is well suited for object detection on mobile devices as well. I trained the face detection model from scratch. Have a look at the **[Training your own Object Detector](#training-your-own-object-detector)** section if you want to train such a model for your own dataset!\r\n\r\n### Pascal VOC\r\n\r\n![voc1](https://user-images.githubusercontent.com/31125521/44733258-7ca2a000-aae7-11e8-93f3-07be9943e222.jpg)\r\n![voc2](https://user-images.githubusercontent.com/31125521/44733259-7d3b3680-aae7-11e8-9794-ab00edef9a48.jpg)\r\n\r\n### COCO\r\n\r\n![coco1](https://user-images.githubusercontent.com/31125521/44733254-7ca2a000-aae7-11e8-9113-28eaea552093.jpg)\r\n![coco2](https://user-images.githubusercontent.com/31125521/44733256-7ca2a000-aae7-11e8-98f6-26853a12248a.jpg)\r\n\r\n### Face Detection\r\n\r\nThe face detection model is one of the models available in [face-api.js](https://github.com/justadudewhohacks/face-api.js).\r\n\r\n![face](https://user-images.githubusercontent.com/31125521/44733257-7ca2a000-aae7-11e8-9ede-1a38f20e36be.jpg)\r\n\r\n<a name=\"running-the-examples\"></a>\r\n\r\n## Running the Examples\r\n\r\n``` bash\r\ncd examples\r\nnpm i\r\nnpm start\r\n```\r\n\r\nBrowse to http://localhost:3000/.\r\n\r\n<a name=\"usage\"></a>\r\n\r\n## Usage\r\n\r\nGet the latest build from dist/tiny-yolov2.js or dist/tiny-yolov2.min.js and include the script:\r\n\r\n``` html\r\n<script src=\"tiny-yolov2.js\"></script>\r\n```\r\n\r\nSimply load the model:\r\n\r\n``` javascript\r\nconst config = // yolo config\r\nconst net = new yolo.TinyYolov2(config)\r\nawait net.load(`voc_model-weights_manifest.json`)\r\n```\r\n\r\nThe config file of the VOC model looks as follows:\r\n\r\n``` js\r\n{\r\n  // the pre trained VOC model uses regular convolutions\r\n  \"withSeparableConvs\": false,\r\n  // iou threshold for nonMaxSuppression\r\n  \"iouThreshold\": 0.4,\r\n  // anchor box dimensions, relative to cell size (32px)\r\n  \"anchors\": [\r\n    { \"x\": 1.08, \"y\": 1.19 },\r\n    { \"x\": 3.42, \"y\": 4.41 },\r\n    { \"x\": 6.63, \"y\": 11.38 },\r\n    { \"x\": 9.42, \"y\": 5.11 },\r\n    { \"x\": 16.62, \"y\": 10.52 }\r\n  ],\r\n  // class labels in correct order\r\n  \"classes\": [\r\n    \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\",\r\n    \"bus\", \"car\", \"cat\", \"chair\", \"cow\",\r\n    \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\",\r\n    \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\r\n  ]\r\n}\r\n```\r\n\r\nInference and drawing the results:\r\n\r\n``` javascript\r\nconst forwardParams = {\r\n  inputSize: 416,\r\n  scoreThreshold: 0.8\r\n}\r\n\r\nconst detections = await net.detect('myInputImage', forwardParams)\r\nyolo.drawDetection('myCanvas', detections)\r\n```\r\n\r\nAlso check out the examples.\r\n\r\n<a name=\"training-your-own-object-detector\"></a>\r\n\r\n## Training your own Object Detector\r\n\r\nIf you want to train your own object detector, I would suggest training a model using separable convolutions, as it will allow for much faster inference times and the training process will converge much faster, as there are significantly less parameters to train.\r\n\r\nTraining a multiclass detector will take quite some time, depending on how much classes you are training your object detector on. However, training a single class detector it is possible to get already pretty good results after training for only a few epochs.\r\n\r\n<a name=\"defining-your-model-config\"></a>\r\n\r\n### Defining your Model Config\r\n\r\n``` js\r\n{\r\n  // use separable convolutions over regular convolutions\r\n  \"withSeparableConvs\": true,\r\n  // iou threshold for nonMaxSuppression\r\n  \"iouThreshold\": 0.4,\r\n  // instructions for how to determine anchors is given below\r\n  \"anchors\": [...],\r\n  // whatever kind of objects you are training your object detector on\r\n  \"classes\": [\"cat\"],\r\n  // optionally you can compute the mean RGB value for your dataset and\r\n  // pass it in the config for performing mean value subtraction on your\r\n  // input images\r\n  \"meanRgb\": [...],\r\n  // scale factors for each loss term (only required for training),\r\n  // explained below\r\n  \"objectScale\": 5,\r\n  \"noObjectScale\": 1,\r\n  \"coordScale\": 1,\r\n  \"classScale\": 1\r\n}\r\n```\r\n\r\n<a name=\"labeling-your-data-with-ground-truth-boxes\"></a>\r\n\r\n### Labeling your Data with Ground Truth Boxes\r\n\r\nFor each image in your training set, you should create a corresponding json file, containing the bounding boxes and class labels of each of the instance of objects located in that image. The bounding box dimensions should be relative to the image dimensions.\r\n\r\nConsider an image with a width and height of 400px, showing a single cat, which is spanned by the bounding box at x = 50px, y = 100px (upper left corner) with a box size of width = 200px and height = 100px. The corresponding json file should look as follows (note, it is an array of all bounding boxes for that image):\r\n\r\n``` json\r\n[\r\n  {\r\n    \"x\": 0.125,\r\n    \"y\": 0.25,\r\n    \"width\": 0.5,\r\n    \"height\": 0.25,\r\n    \"label\": \"cat\"\r\n  }\r\n]\r\n```\r\n\r\n<a name=\"computing-box-anchors\"></a>\r\n\r\n### Computing Box Anchors\r\n\r\nBefore training your detector, you want to compute 5 anchor boxes over your training set. An anchor box is basically an object of shape { \"x\": boxWidth / 32, \"y\": boxHeight / 32 } where x and y are the anchor box sizes relative to the grid cell size (32px).\r\n\r\nTo determine the 5 anchor boxes, you want to simply perform kmeans clustering with 5 clusters over the width and height of each ground truth box of your training set. There should be plenty of options out there, which you can use for kmeans clustering, but I will provide a script for that, coming soon...\r\n\r\n<a name=\"yolo-loss-function\"></a>\r\n\r\n### Yolo Loss Function\r\n\r\nThe Yolo loss function computes the sum of the coordinate, object, class and no object loss. You can tune the weight of each loss term contributing to the totoal loss by adjusting the corresponding scale parameters in your config file, as mentioned above.\r\n\r\nThe no object loss term penalizes the scores of the bounding box of all the box anchors in the grid, which do not have a corresponding ground truth bounding box. In other words, they should optimally predict a score of 0, if there is no object of interest at that position.\r\n\r\nOn the other hand, the object, class and coordinate loss terms refer to the accuracy of the prediction at each anchor position where there is a ground truth bounding box. The coordinate loss simply penalizes the difference between predicted bounding box coordinates and ground truth box coordinates, the object loss penalizes the difference of the predicted confidence score to the box IOU.\r\n\r\nThe class loss penalizes the confidence score of the predicted score. Note, that training a single class object detector you can simply ignore that parameter, as the class loss is always 0 in that case.\r\n\r\nPS: You can simply go with the default values in the above shown config example.\r\n\r\n<a name=\"initializing-the-model-weights\"></a>\r\n\r\n### Initializing the Model Weights\r\n\r\nTraining a model from scratch, you need some weights to begin with. Simply open initWeights.html located in the /train folder of the repo in your browser. Enter the number of classes, hit save and use the saved file as the initial checkpoint weight file.\r\n\r\n<a name=\"start-training\"></a>\r\n\r\n### Start Training\r\n\r\nFor a complete example, also check out the /train folder at the root of this repo, which also contains some tooling to save intermediary checkpoints of your model weights as well as statistics of the average loss after each epoch.\r\n\r\nSet up the model for training:\r\n\r\n``` javascript\r\nconst config = // your config\r\n\r\n// simply use any of the optimizer provided by tfjs (I usually use adam)\r\nconst learningRate = 0.001\r\nconst optimizer = tf.train.adam(learningRate, 0.9, 0.999, 1e-8)\r\n\r\n// initialize a trainable TinyYolov2\r\nconst net = new yolo.TinyYolov2Trainable(config, optimizer)\r\n\r\n// load initial weights or the weights of any checkpoint\r\nconst checkpointUri = 'checkpoints/initial_glorot_1_classes.weights'\r\nconst weights = new Float32Array(await (await fetch(checkpointUri)).arrayBuffer())\r\nawait net.load(weights)\r\n```\r\n\r\nWhat I usually do is naming the json files the same as the corresponding image, e.g. *img1.jpg* and *img1.json* and provide an endpoint to retrieve the json file names as an array:\r\n\r\n``` javascript\r\nconst boxJsonUris = (await fetch('/boxJsonUris')).json()\r\n```\r\n\r\nFurthermore you can choose to train your model on a fixed input size or you can perform multi scale training, which is a good way to improve the accuracy of your model at different scales. This can also be helpful to augment your data, in case you only have a limited number of training samples:\r\n\r\n``` javascript\r\n// should be multiples of 32 (grid cell size)\r\nconst trainingSizes = [160, 224, 320, 416]\r\n```\r\n\r\nThen we can actually train it:\r\n\r\n``` javascript\r\nfor (let epoch = startEpoch; epoch < maxEpoch; epoch++) {\r\n\r\n  // always shuffle your inputs for each epoch\r\n  const shuffledInputs = yolo.shuffleArray(boxJsonUris)\r\n\r\n  // loop through shuffled inputs\r\n  for (let dataIdx = 0; dataIdx < shuffledInputs.length; dataIdx++) {\r\n\r\n    // fetch image and corresponding ground truth bounding boxes\r\n    const boxJsonUri = shuffledInputs[dataIdx]\r\n    const imgUri = boxJsonUri.replace('.json', '.jpg')\r\n\r\n    const groundTruth = await (await fetch(boxJsonUri)).json()\r\n    const img = await yolo.bufferToImage(await (await fetch(imgUri)).blob())\r\n\r\n    // rescale and backward pass input image for each input size\r\n    for (let sizeIdx = 0; sizeIdx < trainSizes.length; sizeIdx++) {\r\n\r\n      const inputSize = trainSizes[sizeIdx]\r\n\r\n      const backwardOptions = {\r\n        // filter boxes with width < 32 or height < 32\r\n        minBoxSize: 32,\r\n        // log computed losses\r\n        reportLosses: function({ losses, numBoxes, inputSize }) {\r\n          console.log(`ground truth boxes: ${numBoxes} (${inputSize})`)\r\n          console.log(`noObjectLoss[${dataIdx}]: ${yolo.round(losses.noObjectLoss, 4)}`)\r\n          console.log(`objectLoss[${dataIdx}]: ${yolo.round(losses.objectLoss, 4)}`)\r\n          console.log(`coordLoss[${dataIdx}]: ${yolo.round(losses.coordLoss, 4)}`)\r\n          console.log(`classLoss[${dataIdx}]: ${yolo.round(losses.classLoss, 4)}`)\r\n          console.log(`totalLoss[${dataIdx}]: ${yolo.round(losses.totalLoss, 4)}`)\r\n        }\r\n      }\r\n\r\n      const loss = await net.backward(img, groundTruth, inputSize, backwardOptions)\r\n\r\n      if (loss) {\r\n        // don't forget to free the loss tensor\r\n        loss.dispose()\r\n      } else {\r\n        console.log('no boxes remaining after filtering')\r\n      }\r\n\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n<a name=\"overfit-first\"></a>\r\n\r\n### Overfit first!\r\n\r\nGenerally it's a good idea, to overfit on a small subset of your training data, to verify, that the loss is converging and that your detector is actually learning something. Therefore, you can simply train your detector on 10 - 20 images of your training data for some epochs. Once the loss converges, save the model, run inference on these 10 - 20 images to view the predicted bounding boxes and compare them to the ground truth boxes.",
  "readmeFilename": "README.md",
  "scripts": {
    "build": "rm -rf ./build && rm -rf ./dist && npm run rollup && npm run rollup-min && npm run tsc && npm run tsc-es6",
    "rollup": "rollup -c rollup.config.js",
    "rollup-min": "rollup -c rollup.config.js --environment minify:true",
    "test": "karma start",
    "test-travis": "set KARMA_BROWSERS=ChromeHeadlessNoSandbox&& karma start --single-run",
    "tsc": "tsc",
    "tsc-es6": "tsc --p tsconfig.es6.json"
  },
  "typings": "./build/commonjs/index.d.ts",
  "version": "0.4.0"
}
